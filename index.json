[{"content":"This is a short article in a series about Lambda Learnings, based on my experience working with Lambdas.\nTL;DR AWS accounts can run up to 1,000 concurrent Lambda instances, but without restrictions, one function could consume the whole pool. Configuring reserved concurrency can prevent this but can lead to surprising behavior on various event sources when the limit is reached and throttling happens. Instead, rate limit the event sources directly.\nThe Unintended Consequences of Reserved Concurrency: \u0026ldquo;Dropped Events\u0026rdquo; By default, an AWS account can run a total of 1,000 concurrent Lambda instances. Without any restrictions, one Lambda function could consume this entire pool, leaving all other functions throttled and unable to handle events, resulting in dropped events (\u0026ldquo;What dropped means is different for each event source and can be quite unexpected as we will see below\u0026rdquo;).\nTo prevent this, you can configure reserved concurrency for each Lambda function. This sets an upper limit on the number of concurrent instances for that specific function.\nHowever, in my experience relying solely on reserved concurrency is not ideal due to the effect \u0026ldquo;Lambda Throttling\u0026rdquo; has on various Lambda event sources. Let\u0026rsquo;s examine the expected versus actual behavior for two common event sources when throttling occurs:\nAPI Gateway Events:\nWhat you might think happens: The client receives a HTTP status code 429 (Rate Limit Exceeded) with a response body explaining the problem. What actually happens: The client receives a HTTP status code 500 (Internal Server Error) with a response body stating \u0026ldquo;Internal Server Error\u0026rdquo;. There is now no way for the client to differentiate this from any other internal server error. SQS Events:\nWhat you might think happens: The message remains in the queue until throttling stops and capacity is available and is then processed. What actually happens: The message is processing fails, it is returned to the queue, and retried based on the redrive policy, retention policy, or sent to another SQS dead-letter queue (DLQ). This could lead to \u0026rsquo;lost\u0026rsquo; messages if not handled appropiately. Although the above behaviour may not seem intuitive, it makes sense because clients are not supposed to be aware of internal resource management issues.\nRecommendation Instead of only setting reserved concurrency on the lambda functions, rate limit the event sources directly to achieve the desired behavior:\nAPI Gateway:\nConfigure rate limits on the endpoints. Customize the status code, header, and body for rate limit responses. By default a HTTP Status Code of 429 (Rate Limit Exceeded) will be sent by API Gateway. SQS Events:\nSet a maximum concurrency on the SQS event source. Once the queue reaches the configured maximum concurrency for the consumer lambdas, no more lambda instances are created and the messages stay in the queue until capacity becomes available. Depending on your use case, it might be useful to use reserved concurrency as a second layer of defense. It can act as a safeguard that should not be reached if the source rate limiting is correctly configured. If it is reached, it signals that you need to adjust your source rate limiting. A CloudWatch alarm can notify you when this limit is approached.\nIf you decide to use reserved concurrency, be sure to test what happens to events when throttling occurs, as the results may surprise you. This behavior is not always clearly documented in the AWS docs in my experience, so some experimentation is necessary.\n","permalink":"https://blog.stefanwaldhauser.me/posts/lambda_learning_rate_limit/","summary":"How reserved concurrency can lead to unintended effects and what to do about it.","title":"Lambda Learning: Prevent High Concurrency by Rate Limiting the Source Instead of Setting Reserved Concurrency"},{"content":"TL;DR In an AWS Lambda setup using application-side database connection pooling, there\u0026rsquo;s a risk of connection leaks due to frozen lambda processes not cleaning up idle database connections. To address this, configure the PostgreSQL idle_session_timeout parameter slightly higher than the application-side pool\u0026rsquo;s idleTimeoutMillis. Also, manually check if your application-side pool contains any idle connections when the lambda process is unfrozen and another execution occurs, to ensure you are not using a database connection that has been terminated by the database while the process was frozen.\nProject Setup The project where the problem occured uses NodeJs based AWS Lambdas that connect to a PostgreSQL AWS RDS database. The main library used for the database handling is the objectâ€“relational mapping tool mikroOrm which uses the SQL query builder knex which in turn uses pg as PostgreSQL client and tarn for application side connection pooling.\nAs recommended by AWS the mikroOrm instance and thus the tarn based database connection pool are created outside of the lambda handler:\nInitialize SDK clients and database connections outside of the function handler (\u0026hellip;) Subsequent invocations processed by the same instance of your function can reuse these resources. This saves cost by reducing function run time.\nThe idea is that variables declared at the global scope within a Node.js module are retained in memory throughout the process\u0026rsquo;s lifespan. Consequently, they can be utilized for multiple invocations of the same lambda instance, persisting even after the handler function concludes its execution. These global variables remain in memory and are not subject to garbage collection.\nWhen the database connection pool is defined as a global variable outside the lambda handler, it persists across invocations of the same lambda instance. This enables reusing database connections within the pool for multiple invocations of the same lambda instance. To avoid keeping connections open indefinitely, the pool allows setting an idleTimeoutMillis. Unused connections are destroyed after the specified duration.\nProblem It was observed that the database connections opened per lambda instance sometimes persisted longer than the expected idleSessionTimeout value set on the application connection pool. This resulted in quickly depleting the number of maximum database connections permitted by our RDS instance.\nThe issue stems from the way tarn (and potentially other pooling libraries) handle the pool reaping process. Reaping involves detecting and clearing idle resources, such as database connections, that remain idle for longer than the specified timeout period.\nThe implementation of reaping in tarn can be seen here:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 _startReaping() { if (!this.interval) { this._executeEventHandlers(\u0026#39;startReaping\u0026#39;); this.interval = setInterval(() =\u0026gt; this.check(), this.reapIntervalMillis); } } check() { const timestamp = now(); const newFree: Resource\u0026lt;T\u0026gt;[] = []; const minKeep = this.min - this.used.length; const maxDestroy = this.free.length - minKeep; let numDestroyed = 0; this.free.forEach(free =\u0026gt; { if ( duration(timestamp, free.timestamp) \u0026gt;= this.idleTimeoutMillis \u0026amp;\u0026amp; numDestroyed \u0026lt; maxDestroy ) { numDestroyed++; this._destroy(free.resource); } else { newFree.push(free); } }); this.free = newFree; // Pool is completely empty, stop reaping. // Next .acquire will start reaping interval again. if (this.isEmpty()) { this._stopReaping(); } } An interval in Node.js is initiated to check every reapIntervalMillis whether any of the idle resources surpass idleTimeoutMillis and subsequently destroys them, thus closing the database connection.\nHowever, the problem arises when a lambda execution concludes. What happens is well explained in this blog post by Dynatrace:\nAfter each execution, AWS Lambda puts the instance to sleep. In other words, the instance freezes (similar to a laptop in hibernate mode). The virtual CPU is turned off. This frees up resources on the worker node. The overhead from waking up such a function is negligible.\nConsequently, the setInterval ceases to run once the execution concludes, and the lambda process enters a frozen state. Therefore, database connections that remain open in the application side pool are not cleaned up after idleTimeoutMillis since the reaping interval is not running. A case of a database connection leak, which degrades database performance.\nSolution The project implements a solution that involves combining application-side pooling with configuring database-side cleanup of idle connections.\nTo implement this solution, follow these steps:\nSet the idle_session_timeout parameter of PostgreSQL to a value slightly higher than the idleTimeoutMillis of your application-side pool. For example, if idleTimeoutMillis is set to 500ms, set idle_session_timeout to 510ms. This ensures that connections missed by the application-side pool while the lambda process was frozen are caught and cleaned up by the database. To ensure connections are always controlled by the application side pool and not unexpectedly terminated by the database while the lambda process is hot, it\u0026rsquo;s crucial to configure the database-level timeout higher than the application sidee timeout.\nWhen using the idle_session_timeout parameter of the database, it\u0026rsquo;s vital to perform an additional manual pool reaping check within the lambda handler before using any connection from the pool. This extra check ensures detection and cleanup of any idle connections within the pool that may have been terminated by the database while the lambda process was frozen. Relying solely on the usual check that runs on a fixed interval isn\u0026rsquo;t sufficient. There\u0026rsquo;s a risk that the reaping check, operating on the interval, kicks in after the first attempt to use a database connection that has already been terminated by the server. With tarn, you can run a manual reaping check by accessing the tarn pool instance and invoking the .check() method. In the project .check() is simply called first thing in the lambda handler before doing anything else, ensuring the application side pool is in sync and does not contain any dead connections.\n","permalink":"https://blog.stefanwaldhauser.me/posts/lambda_db_connection_leak/","summary":"How it happens and what you can do about it.","title":"Application Side Database Connection Pooling can lead to a Connection Leak in an AWS Lambda Application"}]